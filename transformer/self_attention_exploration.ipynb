{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Self Attention Single vs Multi Head Exploration\n",
        "\n",
        "* main goal is to implement single + multi-head self attention"
      ],
      "metadata": {
        "id": "7RzuJ43HWypo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "V--ZucF8X332"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "7nKu_eMQYQ-B"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=-1, keepdims=True)"
      ],
      "metadata": {
        "id": "AQC4ZI08pKrR"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding dim\n",
        "d = 60"
      ],
      "metadata": {
        "id": "f61Mn9PWmKaN"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"i like to eat apples\""
      ],
      "metadata": {
        "id": "ELWKdVgwlVBh"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = sentence.split(\" \")"
      ],
      "metadata": {
        "id": "zeLWNzMal4qP"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7qax72cl698",
        "outputId": "77208247-e756-4080-c422-906b59aa0719"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'like', 'to', 'eat', 'apples']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embedding(d):\n",
        "    return np.random.rand(d)"
      ],
      "metadata": {
        "id": "Dgj1rp-XlbhK"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = np.array([generate_embedding(d) for i in words])"
      ],
      "metadata": {
        "id": "BeU7X4-VlxJB"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOnmUozmmOzJ",
        "outputId": "df710535-ac96-4d5c-f165-880124b010aa"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 60)"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single Head Self Attention"
      ],
      "metadata": {
        "id": "4DZb1u9RX0I6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "tCRBj69VWwFa"
      },
      "outputs": [],
      "source": [
        "_ = \"\"\"\n",
        "\n",
        "ok so to implement this, i just need a sequence of tokens, create \"embeddings\" for them, then do self-attention with those embeddings\n",
        "\n",
        "formula: W = softmax((Q * K.T)/sqrt(d)) * V\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W_q = np.random.rand(d, d)\n",
        "W_k = np.random.rand(d, d)\n",
        "W_v = np.random.rand(d, d)"
      ],
      "metadata": {
        "id": "vYROX7A1lZsK"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = input @ W_q\n",
        "K = input @ W_k\n",
        "V = input @ W_v"
      ],
      "metadata": {
        "id": "daOpfbKLneB4"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Q.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edMNNvEDnsz3",
        "outputId": "6629aa1e-7b96-4d30-9339-ef35190af699"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 60)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = softmax((Q @ K.T) / np.sqrt(d))"
      ],
      "metadata": {
        "id": "jkom8ADzoR2L"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL_E-EDYpF_l",
        "outputId": "af7ef08e-34f0-4bb9-a9f7-a1e9f4c77000"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = W @ input"
      ],
      "metadata": {
        "id": "VMfNgNFkpj0S"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-aYzv_rpl82",
        "outputId": "cb640eda-1180-48e3-eaad-33989b5fa77d"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 60)"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_ = \"\"\" Y represents the output of the self-attention layer \"\"\""
      ],
      "metadata": {
        "id": "RWOOJFyxpw_v"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention"
      ],
      "metadata": {
        "id": "bFklb5jUrF_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h = 4"
      ],
      "metadata": {
        "id": "KhLGvS-MpoaC"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = \"\"\" ok so for mha, we have h: q, k, v \"\"\""
      ],
      "metadata": {
        "id": "-LVPJCjorLS_"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = d // h"
      ],
      "metadata": {
        "id": "4HUBcXKbranl"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_q_heads = [np.random.rand(d, d_k) for _ in range(h)]\n",
        "W_k_heads = [np.random.rand(d, d_k) for _ in range(h)]\n",
        "W_v_heads = [np.random.rand(d, d_k) for _ in range(h)]"
      ],
      "metadata": {
        "id": "NRsvK17lrlcI"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heads_output = []\n",
        "\n",
        "for i in range(h):\n",
        "    Q_i = input @ W_q_heads[i]\n",
        "    K_i = input @ W_k_heads[i]\n",
        "    V_i = input @ W_v_heads[i]\n",
        "\n",
        "    scores = (Q_i @ K_i.T) / np.sqrt(d_k)\n",
        "    A_i = softmax(scores)\n",
        "\n",
        "    head_output = A_i @ V_i\n",
        "    heads_output.append(head_output)"
      ],
      "metadata": {
        "id": "9lDC_ijFrpiQ"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multihead_output = np.concatenate(heads_output, axis=1)"
      ],
      "metadata": {
        "id": "2bjS2H_9r28f"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multihead_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwOaWS9NsKG6",
        "outputId": "9e8ba172-5be3-4033-8ded-6449d8693a73"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 60)"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    }
  ]
}